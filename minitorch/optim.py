from typing import Sequence

from .module import Parameter
from .scalar import Scalar


class Optimizer:
    def __init__(self, parameters: Sequence[Parameter]):
        self.parameters = parameters


class SGD(Optimizer):
    def __init__(self, parameters: Sequence[Parameter], lr: float = 1.0):
        super().__init__(parameters)
        self.lr = lr

    def zero_grad(self) -> None:
        """Zeroes out the gradients of the parameters in the optimizer.

        Iterates over the parameters and sets the derivative and grad values to None if they are not None.

        Returns
        -------
        None

        """
        for p in self.parameters:
            if p.value is None:
                continue
            if hasattr(p.value, "derivative"):
                if p.value.derivative is not None:
                    p.value.derivative = None
            if hasattr(p.value, "grad"):
                if p.value.grad is not None:
                    p.value.grad = None

    def step(self) -> None:
        """Updates the parameters based on the gradient stored in the parameter.

        For each parameter, checks if the derivative or grad is not None, and if so, updates the parameter
        value by subtracting the product of the learning rate and the derivative, or the learning rate and
        the grad. If the parameter is None, or if the derivative or grad is None, the parameter is not
        updated.

        Returns
        -------
        None

        """
        for p in self.parameters:
            if p.value is None:
                continue
            if hasattr(p.value, "derivative"):
                if p.value.derivative is not None:
                    p.update(Scalar(p.value.data - self.lr * p.value.derivative))
            elif hasattr(p.value, "grad"):
                if p.value.grad is not None:
                    p.update(p.value - self.lr * p.value.grad)
